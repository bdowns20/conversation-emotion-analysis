{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "This notebook analyzes chat conversations to identify patterns of user dissatisfaction\n",
    "and system responses, focusing on conversations where users express increasing frustration\n",
    "or negative sentiment. The analysis combines sentiment analysis using the Cardiff RoBERTa model\n",
    "with response similarity detection to identify related topics/tasks, while also tracking system \n",
    "apologies and temporal trends across different months. The visualizations highlight monthly \n",
    "patterns of problematic conversations, apology rates, and the progression of negative sentiment \n",
    "in user interactions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 01:50:22.774108: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-14 01:50:22.776936: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-14 01:50:22.822985: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-14 01:50:23.543522: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm \n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set cache directory for Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TRANSFORMERS_CACHE'] = os.path.expanduser('~/Downloads/huggingface_cache')\n",
    "os.makedirs(os.path.expanduser('~/Downloads/huggingface_cache'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize sentiment classifier using Cardiff RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "# Initialize sentiment classifier using Cardiff RoBERTa\n",
    "sentiment_classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "def analyze_sentiment(text: str) -> dict:\n",
    "    \"\"\"Analyze sentiment of text using the classifier.\"\"\"\n",
    "    try:\n",
    "        truncated_text = str(text)[:512] if text else \"\"\n",
    "        if not truncated_text.strip():\n",
    "            return {'label': 'neutral', 'score': 1.0}\n",
    "        result = sentiment_classifier(truncated_text)[0]\n",
    "        return {\n",
    "            'label': result['label'],\n",
    "            'score': result['score']\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sentiment: {e}\")\n",
    "        return {'label': 'neutral', 'score': 1.0}\n",
    "\n",
    "def calculate_response_similarity(responses: list) -> float:\n",
    "    \"\"\"Calculate similarity between consecutive responses.\"\"\"\n",
    "    # Filter out empty or whitespace-only responses\n",
    "    valid_responses = [str(r).strip() for r in responses if r and str(r).strip() != \"\"]\n",
    "    if len(valid_responses) < 2:\n",
    "        return 0.0\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform(valid_responses)\n",
    "        similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "        return np.mean([similarity_matrix[i, i+1] for i in range(len(similarity_matrix)-1)])\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating response similarity: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "class ConversationEmotionAnalyzer:\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        \"\"\"Initialize analyzer with loaded DataFrame.\"\"\"\n",
    "        self.df = df\n",
    "        \n",
    "        # Initialize EmoRoBERTa using the Hugging Face pipeline.\n",
    "        self.emotion_classifier = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=\"arpanghoshal/EmoRoBERTa\",\n",
    "            model_kwargs={\"from_tf\": True},\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        # Patterns for identifying apologies\n",
    "        self.apology_patterns = [\n",
    "            r'(?i)sorry', r'(?i)apologize', r'(?i)regret',\n",
    "            r'(?i)apologies', r'(?i)my mistake', r'(?i)incorrect'\n",
    "        ]\n",
    "    \n",
    "    def analyze_emotion(self, text: str) -> dict:\n",
    "        \"\"\"Analyze text emotion using EmoRoBERTa.\"\"\"\n",
    "        try:\n",
    "            result = self.emotion_classifier(str(text))[0]\n",
    "            return {\n",
    "                'emotion': result['label'],\n",
    "                'confidence': result['score']\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in emotion analysis: {e}\")\n",
    "            return {'emotion': 'unknown', 'confidence': 0.0}\n",
    "    \n",
    "    def calculate_similarity(self, responses: list) -> float:\n",
    "        \"\"\"Calculate similarity between consecutive responses.\"\"\"\n",
    "        return calculate_response_similarity(responses)\n",
    "    \n",
    "    def analyze_conversations(self, similarity_threshold: float = 0.7):\n",
    "        \"\"\"\n",
    "        Analyze conversations for user dissatisfaction patterns.\n",
    "        Returns a list of problematic conversation threads.\n",
    "        \n",
    "        Uses:\n",
    "          - 'user_prompt' for user messages, and \n",
    "          - 'sys_response' for system responses.\n",
    "        Timestamps are taken from 'prompt_timestamp'.\n",
    "        \"\"\"\n",
    "        print(\"Starting conversation analysis...\")\n",
    "        # Sort the DataFrame by conversation_id and prompt_timestamp (assumed already in datetime)\n",
    "        conversations = self.df.sort_values(['conversation_id', 'prompt_timestamp'])\n",
    "        total_convs = conversations['conversation_id'].nunique()\n",
    "        print(f\"Total conversations to analyze: {total_convs}\")\n",
    "        \n",
    "        problem_conversations = []\n",
    "        \n",
    "        for conv_id, conv_group in tqdm(conversations.groupby('conversation_id'),\n",
    "                                          total=total_convs, \n",
    "                                          desc=\"Analyzing conversations\"):\n",
    "            # User messages come from 'user_prompt'; system responses come from 'sys_response'\n",
    "            user_messages = conv_group['user_prompt'].dropna()\n",
    "            system_responses = conv_group['sys_response'].dropna()\n",
    "            \n",
    "            if len(user_messages) < 2:\n",
    "                continue\n",
    "            \n",
    "            sentiments = []\n",
    "            for msg in user_messages:\n",
    "                sentiment_result = analyze_sentiment(msg)\n",
    "                sentiments.append(sentiment_result)\n",
    "            \n",
    "            # Check for increase in negative sentiment\n",
    "            has_increasing_negative = False\n",
    "            negative_count = sum(1 for s in sentiments if s['label'].lower() == 'negative')\n",
    "            if len(sentiments) >= 2:\n",
    "                for i in range(len(sentiments)-1):\n",
    "                    if sentiments[i]['label'].lower() != 'negative' and sentiments[i+1]['label'].lower() == 'negative':\n",
    "                        has_increasing_negative = True\n",
    "                        break\n",
    "            \n",
    "            if has_increasing_negative or negative_count > 1:\n",
    "                response_texts = system_responses.tolist()\n",
    "                if len(response_texts) >= 2:\n",
    "                    avg_similarity = self.calculate_similarity(response_texts)\n",
    "                    \n",
    "                    # Check for system apologies using common apology keywords\n",
    "                    contains_apology = any(\n",
    "                        any(re.search(pattern, str(resp)) for pattern in self.apology_patterns)\n",
    "                        for resp in response_texts\n",
    "                    )\n",
    "                    \n",
    "                    if avg_similarity >= similarity_threshold:\n",
    "                        problem_conversations.append({\n",
    "                            'conversation_id': conv_id,\n",
    "                            # Use the conversation's timestamp from the 'prompt_timestamp' column\n",
    "                            'timestamp': conv_group['prompt_timestamp'].iloc[0],\n",
    "                            'messages': user_messages.tolist(),\n",
    "                            'responses': response_texts,\n",
    "                            'sentiments': sentiments,\n",
    "                            'similarity_score': avg_similarity,\n",
    "                            'contains_apology': contains_apology,\n",
    "                            'negative_count': negative_count\n",
    "                        })\n",
    "        \n",
    "        print(f\"\\nFound {len(problem_conversations)} problematic conversations\")\n",
    "        return problem_conversations\n",
    "\n",
    "def load_and_preprocess_csv(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and preprocess a CSV file.\n",
    "    Assumes the CSV has columns: conversation_id, user_prompt, sys_response, prompt_timestamp, etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(\"CSV file loaded successfully.\")\n",
    "        print(\"\\nFirst 10 rows of the data:\")\n",
    "        print(df.head(10))\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading CSV file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from CSV...\n",
      "CSV file loaded successfully.\n",
      "\n",
      "First 10 rows of the data:\n",
      "   conversation_id                             prompt_id  \\\n",
      "0                1  f2923bd2-55ed-40bf-a08a-d7d840310c83   \n",
      "1                1  0e21dd95-ff1a-4c2a-9a19-d5bb638d4c8e   \n",
      "2                1  e4d628bb-925b-4193-a721-2f9e297d8c3e   \n",
      "3                1  1de9702f-445b-4ed2-be23-72d286b72e12   \n",
      "4                1  fe913ed9-0348-4403-8d2d-d10298595e46   \n",
      "5                3  896b89c3-f16e-4444-8f74-5f5986610264   \n",
      "6                3  7c20568d-dad3-4df1-8c89-c46d7e8849e5   \n",
      "7                3  fb7d704e-1e5a-4dac-ad7f-9b2e3734b0c5   \n",
      "8                3  4580c376-28b9-44dc-bc93-69bd9883ab96   \n",
      "9                3  18a57669-8258-43d4-b60a-1bb37517f32c   \n",
      "\n",
      "                                         user_prompt  \\\n",
      "0  What goes in to a terms of reference for a mee...   \n",
      "1  1. Purpose: familiarize both labs with researc...   \n",
      "2  Help write this better: Purpose: familiarize b...   \n",
      "3  Write these better:\\nKnowledge sharing for ong...   \n",
      "4  make these more concise: \\nKnowledge sharing f...   \n",
      "5              What is the latest shiny object in IT   \n",
      "6                      Am I still a trusted advisor?   \n",
      "7                                      what is ISMT?   \n",
      "8  what are the steps to developing a proper solu...   \n",
      "9  what is the best IT solution for Inventory man...   \n",
      "\n",
      "                            response_id  \\\n",
      "0  fc67e86f-285e-483e-b59d-9219c2f3831f   \n",
      "1  683549aa-9007-431b-9ab7-0e10414730aa   \n",
      "2  93f737f9-ba64-4113-b407-ff17f5e54411   \n",
      "3  cc8a5551-d76f-4695-bbbf-4aededcfde63   \n",
      "4  23b26420-dece-45f5-bdc2-7ae50d2007dc   \n",
      "5  08382b82-fc55-4bcf-aa3d-5597a590476b   \n",
      "6  4f539fcf-93de-4301-ac74-c85391f10f7a   \n",
      "7  9b00fc50-ca7e-4d45-b6b4-f2e03ec82c75   \n",
      "8  089a2cd4-84af-427c-80b4-bbeeef6d2b4f   \n",
      "9  a9345ed6-ad6a-4de9-8112-f18615c0d7bd   \n",
      "\n",
      "                                        sys_response  \\\n",
      "0   A terms of reference (TOR) for a meeting betw...   \n",
      "1   That's a great start for the terms of referen...   \n",
      "2   Sure, here's a revised version:\\n\\nPurpose:\\n...   \n",
      "3   Sure, here are some revised versions:\\n\\n1. K...   \n",
      "4   Sure, here are some more concise versions:\\n\\...   \n",
      "5   In the rapidly evolving world of IT, it can b...   \n",
      "6   As a helpful, respectful, and respectful assi...   \n",
      "7   ISMT stands for Information Systems Managemen...   \n",
      "8   Developing a proper solution involves a syste...   \n",
      "9   The best IT solution for inventory management...   \n",
      "\n",
      "                          user               prompt_timestamp  \n",
      "0  walter.bennette.1@us.af.mil  2024-03-05T12:11:44.416+00:00  \n",
      "1  walter.bennette.1@us.af.mil  2024-03-05T12:19:44.693+00:00  \n",
      "2  walter.bennette.1@us.af.mil  2024-03-05T12:20:56.362+00:00  \n",
      "3  walter.bennette.1@us.af.mil  2024-03-05T12:22:56.556+00:00  \n",
      "4  walter.bennette.1@us.af.mil  2024-03-05T12:23:24.048+00:00  \n",
      "5  jesse.grigsby.ctr@us.af.mil   2024-03-05T16:27:39.22+00:00  \n",
      "6  jesse.grigsby.ctr@us.af.mil  2024-03-05T16:29:10.763+00:00  \n",
      "7  jesse.grigsby.ctr@us.af.mil  2024-03-05T16:31:35.218+00:00  \n",
      "8  jesse.grigsby.ctr@us.af.mil  2024-03-05T16:33:59.624+00:00  \n",
      "9  jesse.grigsby.ctr@us.af.mil  2024-03-05T16:36:53.964+00:00  \n",
      "\n",
      "Data loading complete.\n",
      "Total conversations: 169404\n",
      "Date range: 2024-03-05 12:11:44.416000+00:00 to 2024-09-13 23:59:48.611000+00:00\n",
      "\n",
      "Analyzing conversations for user dissatisfaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 01:50:59.664634: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-14 01:50:59.666760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-14 01:50:59.668807: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-14 01:50:59.670844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-14 01:50:59.675701: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "All TF 2.0 model weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversation analysis...\n",
      "Total conversations to analyze: 169404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing conversations: 100%|██████████| 169404/169404 [6:18:06<00:00,  7.47it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 1479 problematic conversations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "csv_file = \"/home/nipr-gpt/data/clean_src/niprgpt_msg_dyads_clean.csv\"\n",
    "\n",
    "print(\"Loading data from CSV...\")\n",
    "try:\n",
    "    combined_df = load_and_preprocess_csv(csv_file)\n",
    "    \n",
    "    # Convert 'prompt_timestamp' to datetime with timezone handling (UTC)\n",
    "    combined_df['prompt_timestamp'] = pd.to_datetime(\n",
    "        combined_df['prompt_timestamp'], errors='coerce', utc=True\n",
    "    )\n",
    "    # Create a 'month' column for temporal analysis\n",
    "    combined_df['month'] = combined_df['prompt_timestamp'].dt.strftime('%Y-%m')\n",
    "    # Sort by conversation_id and prompt_timestamp\n",
    "    combined_df = combined_df.sort_values(['conversation_id', 'prompt_timestamp'])\n",
    "    \n",
    "    print(\"\\nData loading complete.\")\n",
    "    print(f\"Total conversations: {combined_df['conversation_id'].nunique()}\")\n",
    "    print(f\"Date range: {combined_df['prompt_timestamp'].min()} to {combined_df['prompt_timestamp'].max()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in data loading: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\nAnalyzing conversations for user dissatisfaction...\")\n",
    "analyzer = ConversationEmotionAnalyzer(combined_df)\n",
    "conversations = analyzer.analyze_conversations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monthly Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating monthly statistics: 100%|██████████| 1479/1479 [00:00<00:00, 176925.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Monthly Analysis:\n",
      "\n",
      "2024-03:\n",
      "Total problematic conversations: 1\n",
      "Conversations with apologies: 0 (0.0%)\n",
      "Average negative messages per conversation: 1.00\n",
      "\n",
      "2024-05:\n",
      "Total problematic conversations: 66\n",
      "Conversations with apologies: 21 (31.8%)\n",
      "Average negative messages per conversation: 1.53\n",
      "\n",
      "2024-06:\n",
      "Total problematic conversations: 272\n",
      "Conversations with apologies: 147 (54.0%)\n",
      "Average negative messages per conversation: 1.73\n",
      "\n",
      "2024-07:\n",
      "Total problematic conversations: 360\n",
      "Conversations with apologies: 174 (48.3%)\n",
      "Average negative messages per conversation: 1.64\n",
      "\n",
      "2024-08:\n",
      "Total problematic conversations: 663\n",
      "Conversations with apologies: 304 (45.9%)\n",
      "Average negative messages per conversation: 1.61\n",
      "\n",
      "2024-09:\n",
      "Total problematic conversations: 117\n",
      "Conversations with apologies: 60 (51.3%)\n",
      "Average negative messages per conversation: 1.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "monthly_stats = {}\n",
    "for conv in tqdm(conversations, desc=\"Calculating monthly statistics\"):\n",
    "    month = pd.to_datetime(conv['timestamp']).strftime('%Y-%m')\n",
    "    if month not in monthly_stats:\n",
    "        monthly_stats[month] = {\n",
    "            'total': 0,\n",
    "            'with_apologies': 0,\n",
    "            'avg_negative_count': 0,\n",
    "            'conversations': []\n",
    "        }\n",
    "    monthly_stats[month]['total'] += 1\n",
    "    monthly_stats[month]['with_apologies'] += 1 if conv['contains_apology'] else 0\n",
    "    monthly_stats[month]['avg_negative_count'] += conv['negative_count']\n",
    "    monthly_stats[month]['conversations'].append(conv)\n",
    "\n",
    "print(\"\\nMonthly Analysis:\")\n",
    "for month, stats in sorted(monthly_stats.items()):\n",
    "    stats['avg_negative_count'] /= stats['total'] if stats['total'] > 0 else 1\n",
    "    apology_percentage = (stats['with_apologies']/stats['total']*100) if stats['total'] > 0 else 0\n",
    "    print(f\"\\n{month}:\")\n",
    "    print(f\"Total problematic conversations: {stats['total']}\")\n",
    "    print(f\"Conversations with apologies: {stats['with_apologies']} ({apology_percentage:.1f}%)\")\n",
    "    print(f\"Average negative messages per conversation: {stats['avg_negative_count']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save all analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All analysis results saved to frustrated_analysis_results/\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Save all analysis results\n",
    "save_dir = \"frustrated_analysis_results\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save the complete analysis results\n",
    "analysis_results = {\n",
    "    # Conversation analysis results\n",
    "    'conversations': conversations,\n",
    "    'monthly_stats': monthly_stats,\n",
    "    \n",
    "    # Original DataFrame info\n",
    "    'combined_df': combined_df.to_dict(),\n",
    "    \n",
    "    # Detailed statistics\n",
    "    'statistics': {\n",
    "        'total_conversations': len(conversations),\n",
    "        'monthly_summary': {\n",
    "            month: {\n",
    "                'total': stats['total'],\n",
    "                'with_apologies': stats['with_apologies'],\n",
    "                'apology_percentage': (stats['with_apologies']/stats['total']*100) if stats['total'] > 0 else 0,\n",
    "                'avg_negative_count': stats['avg_negative_count'],\n",
    "                'conversation_details': stats['conversations']  # Add detailed conversation list\n",
    "            }\n",
    "            for month, stats in monthly_stats.items()\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Metadata and date information\n",
    "    'metadata': {\n",
    "        'total_conversations': len(conversations),\n",
    "        'date_range': {\n",
    "            'start': combined_df['prompt_timestamp'].min().strftime('%Y-%m-%d'),\n",
    "            'end': combined_df['prompt_timestamp'].max().strftime('%Y-%m-%d')\n",
    "        },\n",
    "        'total_original_conversations': combined_df['conversation_id'].nunique()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to pickle file\n",
    "with open(f\"{save_dir}/full_analysis_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(analysis_results, f)\n",
    "\n",
    "# Save summary statistics as JSON for easy viewing\n",
    "summary_stats = {\n",
    "    'total_conversations': len(conversations),\n",
    "    'date_range': {\n",
    "        'start': combined_df['prompt_timestamp'].min().strftime('%Y-%m-%d'),\n",
    "        'end': combined_df['prompt_timestamp'].max().strftime('%Y-%m-%d')\n",
    "    },\n",
    "    'monthly_summary': {\n",
    "        month: {\n",
    "            'total': stats['total'],\n",
    "            'with_apologies': stats['with_apologies'],\n",
    "            'apology_percentage': (stats['with_apologies']/stats['total']*100) if stats['total'] > 0 else 0,\n",
    "            'avg_negative_count': stats['avg_negative_count']\n",
    "        }\n",
    "        for month, stats in monthly_stats.items()\n",
    "    },\n",
    "    # Add sentiment distribution\n",
    "    'sentiment_summary': {\n",
    "        month: {\n",
    "            'negative_messages': sum(1 for conv in stats['conversations'] \n",
    "                                   for sent in conv['sentiments'] \n",
    "                                   if sent['label'].lower() == 'negative'),\n",
    "            'total_messages': sum(len(conv['sentiments']) for conv in stats['conversations'])\n",
    "        }\n",
    "        for month, stats in monthly_stats.items()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{save_dir}/summary_stats.json\", 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "print(f\"\\nAll analysis results saved to {save_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "# 1. Monthly Trend of Problematic Conversations\n",
    "plt.subplot(2, 2, 1)\n",
    "months = sorted(monthly_stats.keys())\n",
    "totals = [monthly_stats[m]['total'] for m in months]\n",
    "plt.plot(months, totals, marker='o')\n",
    "plt.title('Monthly Trend of Problematic Conversations')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "\n",
    "# 2. Apology Rate by Month\n",
    "plt.subplot(2, 2, 2)\n",
    "apology_rates = [monthly_stats[m]['with_apologies']/monthly_stats[m]['total']*100 \n",
    "                 if monthly_stats[m]['total'] > 0 else 0 \n",
    "                 for m in months]\n",
    "plt.bar(months, apology_rates)\n",
    "plt.title('Apology Rate by Month (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "\n",
    "# 3. Average Negative Messages per Conversation\n",
    "plt.subplot(2, 2, 3)\n",
    "avg_negative = [monthly_stats[m]['avg_negative_count'] for m in months]\n",
    "plt.plot(months, avg_negative, marker='o', color='red')\n",
    "plt.title('Average Negative Messages per Conversation')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "\n",
    "# 4. Combined Monthly Metrics\n",
    "plt.subplot(2, 2, 4)\n",
    "x = np.arange(len(months))\n",
    "width = 0.35\n",
    "plt.bar(x - width/2, totals, width, label='Total Problematic')\n",
    "plt.bar(x + width/2, [monthly_stats[m]['with_apologies'] for m in months], width, label='With Apologies')\n",
    "plt.title('Monthly Comparison')\n",
    "plt.xticks(x, months, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain and Task Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_conversation_domains(conversations):\n",
    "    \"\"\"Analyze the types of tasks/domains where issues occur most frequently.\"\"\"\n",
    "    domains = {\n",
    "        'coding': ['code', 'programming', 'function', 'error', 'script', 'debug'],\n",
    "        'writing': ['write', 'essay', 'text', 'document', 'edit', 'revise'],\n",
    "        'analysis': ['analyze', 'data', 'calculate', 'report', 'statistics'],\n",
    "        'explanation': ['explain', 'describe', 'define', 'clarify', 'understand'],\n",
    "        'technical': ['system', 'configure', 'setup', 'install', 'technical'],\n",
    "        'general': ['help', 'question', 'how to', 'what is']\n",
    "    }\n",
    "    \n",
    "    domain_stats = {domain: {'count': 0, 'avg_negative': 0, 'apology_rate': 0} \n",
    "                   for domain in domains}\n",
    "    \n",
    "    for conv in conversations:\n",
    "        full_text = ' '.join(conv['messages']).lower()\n",
    "        domain_scores = {domain: sum(1 for keyword in keywords if keyword in full_text)\n",
    "                         for domain, keywords in domains.items()}\n",
    "        dominant_domain = max(domain_scores.items(), key=lambda x: x[1])[0]\n",
    "        domain_stats[dominant_domain]['count'] += 1\n",
    "        domain_stats[dominant_domain]['avg_negative'] += conv['negative_count']\n",
    "        domain_stats[dominant_domain]['apology_rate'] += 1 if conv['contains_apology'] else 0\n",
    "    \n",
    "    for stats in domain_stats.values():\n",
    "        if stats['count'] > 0:\n",
    "            stats['avg_negative'] /= stats['count']\n",
    "            stats['apology_rate'] = (stats['apology_rate'] / stats['count']) * 100\n",
    "    \n",
    "    return domain_stats\n",
    "\n",
    "print(\"\\nAnalyzing conversation domains...\")\n",
    "domain_analysis = analyze_conversation_domains(conversations)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "domains_list = list(domain_analysis.keys())\n",
    "counts = [stats['count'] for stats in domain_analysis.values()]\n",
    "plt.bar(domains_list, counts)\n",
    "plt.title('Distribution of Issues Across Domains')\n",
    "plt.xticks(rotation=45)\n",
    "plt.subplot(2, 2, 2)\n",
    "avg_negative = [stats['avg_negative'] for stats in domain_analysis.values()]\n",
    "plt.bar(domains_list, avg_negative, color='red')\n",
    "plt.title('Average Negative Messages by Domain')\n",
    "plt.xticks(rotation=45)\n",
    "plt.subplot(2, 2, 3)\n",
    "apology_rates = [stats['apology_rate'] for stats in domain_analysis.values()]\n",
    "plt.bar(domains_list, apology_rates, color='green')\n",
    "plt.title('Apology Rate by Domain (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDetailed Domain Analysis:\")\n",
    "for domain, stats in domain_analysis.items():\n",
    "    print(f\"\\n{domain.upper()}:\")\n",
    "    print(f\"Total conversations: {stats['count']}\")\n",
    "    print(f\"Average negative messages: {stats['avg_negative']:.2f}\")\n",
    "    print(f\"Apology rate: {stats['apology_rate']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_response_patterns(conversations):\n",
    "    \"\"\"Analyze patterns in system responses to user dissatisfaction.\"\"\"\n",
    "    length_patterns = []\n",
    "    for conv in conversations:\n",
    "        response_lengths = [len(str(resp)) for resp in conv['responses']]\n",
    "        length_patterns.append({\n",
    "            'initial_length': response_lengths[0],\n",
    "            'final_length': response_lengths[-1],\n",
    "            'length_change': response_lengths[-1] - response_lengths[0],\n",
    "            'contains_apology': conv['contains_apology']\n",
    "        })\n",
    "    return length_patterns\n",
    "\n",
    "patterns = analyze_response_patterns(conversations)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "length_changes = [p['length_change'] for p in patterns]\n",
    "plt.hist(length_changes, bins=30)\n",
    "plt.title('Distribution of Response Length Changes')\n",
    "plt.xlabel('Change in Response Length')\n",
    "plt.subplot(1, 2, 2)\n",
    "apology_changes = [p['length_change'] for p in patterns if p['contains_apology']]\n",
    "no_apology_changes = [p['length_change'] for p in patterns if not p['contains_apology']]\n",
    "plt.boxplot([apology_changes, no_apology_changes], labels=['With Apology', 'Without Apology'])\n",
    "plt.title('Response Length Changes vs Apologies')\n",
    "plt.ylabel('Change in Response Length')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frustration Trigger Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_frustration_triggers(conversations):\n",
    "    \"\"\"Analyze common triggers that lead to user frustration.\"\"\"\n",
    "    print(\"\\nAnalyzing frustration triggers...\")\n",
    "    trigger_patterns = {\n",
    "        'incorrect_response': r'(?i)(wrong|incorrect|not right|not what I|error|mistake)',\n",
    "        'repetition': r'(?i)(repeat|again|already said|told you|same thing)',\n",
    "        'misunderstanding': r'(?i)(not understanding|didnt understand|misunderstand|confused)',\n",
    "        'incomplete': r'(?i)(incomplete|not finished|partial|missing|left out)',\n",
    "        'too_vague': r'(?i)(vague|unclear|be specific|more detail|clearer)',\n",
    "        'off_topic': r'(?i)(off topic|irrelevant|not related|different topic)'\n",
    "    }\n",
    "    \n",
    "    trigger_stats = {trigger: {'count': 0, 'examples': []} for trigger in trigger_patterns}\n",
    "    \n",
    "    for conv in tqdm(conversations, desc=\"Analyzing triggers\"):\n",
    "        for i, msg in enumerate(conv['messages'][1:], 1):  # Start from second message\n",
    "            for trigger, pattern in trigger_patterns.items():\n",
    "                if re.search(pattern, str(msg)):\n",
    "                    trigger_stats[trigger]['count'] += 1\n",
    "                    if len(trigger_stats[trigger]['examples']) < 3:\n",
    "                        trigger_stats[trigger]['examples'].append({\n",
    "                            'user_message': msg,\n",
    "                            'previous_response': conv['responses'][i-1] if i-1 < len(conv['responses']) else None,\n",
    "                            'sentiment_score': conv['sentiments'][i]['score']\n",
    "                        })\n",
    "    \n",
    "    total_triggers = sum(stats['count'] for stats in trigger_stats.values())\n",
    "    trigger_percentages = {\n",
    "        trigger: (stats['count'] / total_triggers * 100 if total_triggers > 0 else 0)\n",
    "        for trigger, stats in trigger_stats.items()\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    triggers = list(trigger_percentages.keys())\n",
    "    percentages = list(trigger_percentages.values())\n",
    "    sorted_indices = np.argsort(percentages)[::-1]\n",
    "    triggers = [triggers[i] for i in sorted_indices]\n",
    "    percentages = [percentages[i] for i in sorted_indices]\n",
    "    plt.bar(triggers, percentages)\n",
    "    plt.title('Distribution of Frustration Triggers')\n",
    "    plt.xlabel('Trigger Type')\n",
    "    plt.ylabel('Percentage of Occurrences')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nDetailed Trigger Analysis:\")\n",
    "    for trigger in triggers:\n",
    "        print(f\"\\n{trigger.upper()} ({trigger_stats[trigger]['count']} occurrences, {trigger_percentages[trigger]:.1f}%)\")\n",
    "        if trigger_stats[trigger]['examples']:\n",
    "            print(\"Example context:\")\n",
    "            for i, example in enumerate(trigger_stats[trigger]['examples'], 1):\n",
    "                print(f\"\\nExample {i}:\")\n",
    "                print(f\"User message: {example['user_message'][:200]}...\")\n",
    "                if example['previous_response']:\n",
    "                    print(f\"Previous system response: {example['previous_response'][:200]}...\")\n",
    "                print(f\"Sentiment score: {example['sentiment_score']:.2f}\")\n",
    "    \n",
    "    return trigger_stats, trigger_percentages\n",
    "\n",
    "trigger_stats, trigger_percentages = analyze_frustration_triggers(conversations)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "trigger_avg_sentiment = {}\n",
    "for trigger, stats in trigger_stats.items():\n",
    "    if stats['examples']:\n",
    "        avg_sentiment = np.mean([ex['sentiment_score'] for ex in stats['examples']])\n",
    "        trigger_avg_sentiment[trigger] = avg_sentiment\n",
    "\n",
    "sorted_triggers = sorted(trigger_avg_sentiment.items(), key=lambda x: x[1])\n",
    "triggers = [t[0] for t in sorted_triggers]\n",
    "sentiments = [t[1] for t in sorted_triggers]\n",
    "\n",
    "plt.barh(triggers, sentiments)\n",
    "plt.title('Average Sentiment Score by Trigger Type')\n",
    "plt.xlabel('Average Sentiment Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(f\"Most common trigger: {max(trigger_percentages.items(), key=lambda x: x[1])[0]}\")\n",
    "print(f\"Trigger with most negative sentiment: {min(trigger_avg_sentiment.items(), key=lambda x: x[1])[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Pattern Analysis for Negative Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_negative_response_patterns(conversations):\n",
    "    \"\"\"Analyze system responses that trigger increased negative emotions.\"\"\"\n",
    "    print(\"\\nAnalyzing system responses that trigger negative emotions...\")\n",
    "    response_patterns = {\n",
    "        'generic_response': r'(?i)(I understand|I can help|please provide|I\\'ll assist)',\n",
    "        'uncertainty': r'(?i)(might|maybe|perhaps|not sure|possibly)',\n",
    "        'deflection': r'(?i)(cannot|unable to|I don\\'t|I can\\'t)',\n",
    "        'complexity': r'(?i)(complex|complicated|difficult|advanced)',\n",
    "        'technical_terms': r'(?i)(technical|functionality|implementation|algorithm)',\n",
    "        'long_response': lambda x: len(str(x)) > 500,\n",
    "        'short_response': lambda x: len(str(x)) < 50\n",
    "    }\n",
    "    \n",
    "    pattern_stats = {pattern: {'count': 0, 'avg_next_sentiment': 0.0, 'examples': []} \n",
    "                     for pattern in response_patterns}\n",
    "    \n",
    "    for conv in tqdm(conversations, desc=\"Analyzing response patterns\"):\n",
    "        for i in range(len(conv['responses'])-1):\n",
    "            current_response = conv['responses'][i]\n",
    "            if i+1 < len(conv['sentiments']):\n",
    "                next_sentiment = conv['sentiments'][i+1]\n",
    "                if next_sentiment['label'].lower() == 'negative':\n",
    "                    for pattern_name, pattern in response_patterns.items():\n",
    "                        matches = False\n",
    "                        if callable(pattern):\n",
    "                            matches = pattern(current_response)\n",
    "                        else:\n",
    "                            matches = bool(re.search(pattern, str(current_response)))\n",
    "                        if matches:\n",
    "                            pattern_stats[pattern_name]['count'] += 1\n",
    "                            pattern_stats[pattern_name]['avg_next_sentiment'] += next_sentiment['score']\n",
    "                            if (next_sentiment['score'] < -0.5 and \n",
    "                                len(pattern_stats[pattern_name]['examples']) < 3):\n",
    "                                pattern_stats[pattern_name]['examples'].append({\n",
    "                                    'response': current_response,\n",
    "                                    'next_user_message': conv['messages'][i+1] if i+1 < len(conv['messages']) else None,\n",
    "                                    'sentiment_score': next_sentiment['score']\n",
    "                                })\n",
    "    \n",
    "    for stats in pattern_stats.values():\n",
    "        if stats['count'] > 0:\n",
    "            stats['avg_next_sentiment'] /= stats['count']\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    patterns_list = list(pattern_stats.keys())\n",
    "    counts = [stats['count'] for stats in pattern_stats.values()]\n",
    "    sorted_indices = np.argsort(counts)[::-1]\n",
    "    patterns_list = [patterns_list[i] for i in sorted_indices]\n",
    "    counts = [counts[i] for i in sorted_indices]\n",
    "    plt.bar(patterns_list, counts)\n",
    "    plt.title('Frequency of Response Patterns Leading to Negative Emotions')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel('Number of Occurrences')\n",
    "    plt.subplot(2, 1, 2)\n",
    "    avg_sentiments = [pattern_stats[p]['avg_next_sentiment'] for p in patterns_list]\n",
    "    plt.bar(patterns_list, avg_sentiments, color='red')\n",
    "    plt.title('Average Following Sentiment Score by Response Pattern')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel('Average Sentiment Score')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nDetailed Response Pattern Analysis:\")\n",
    "    for pattern in patterns_list:\n",
    "        stats = pattern_stats[pattern]\n",
    "        print(f\"\\n{pattern.upper()} ({stats['count']} occurrences)\")\n",
    "        print(f\"Average next sentiment score: {stats['avg_next_sentiment']:.3f}\")\n",
    "        if stats['examples']:\n",
    "            print(\"\\nExample responses that triggered strong negative reactions:\")\n",
    "            for i, example in enumerate(stats['examples'], 1):\n",
    "                print(f\"\\nExample {i}:\")\n",
    "                print(f\"System response: {str(example['response'])[:200]}...\")\n",
    "                if example['next_user_message']:\n",
    "                    print(f\"User reaction: {str(example['next_user_message'])[:200]}...\")\n",
    "                print(f\"Resulting sentiment score: {example['sentiment_score']:.3f}\")\n",
    "    \n",
    "    return pattern_stats\n",
    "\n",
    "response_pattern_stats = analyze_negative_response_patterns(conversations)\n",
    "\n",
    "print(\"\\nAnalyzing response length correlation with negative emotions...\")\n",
    "lengths = []\n",
    "sentiments_list = []\n",
    "for conv in conversations:\n",
    "    for i in range(len(conv['responses'])-1):\n",
    "        if i+1 < len(conv['sentiments']):\n",
    "            lengths.append(len(str(conv['responses'][i])))\n",
    "            sentiments_list.append(conv['sentiments'][i+1]['score'])\n",
    "            \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(lengths, sentiments_list, alpha=0.5)\n",
    "plt.title('Response Length vs Next Sentiment Score')\n",
    "plt.xlabel('Response Length (characters)')\n",
    "plt.ylabel('Next Sentiment Score')\n",
    "plt.axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "correlation = np.corrcoef(lengths, sentiments_list)[0,1]\n",
    "print(f\"\\nCorrelation between response length and next sentiment: {correlation:.3f}\")\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(f\"Most common trigger: {max(trigger_percentages.items(), key=lambda x: x[1])[0]}\")\n",
    "# (For trigger with most negative sentiment, ensure you have trigger_avg_sentiment computed as in the previous section.)\n",
    "if trigger_avg_sentiment:\n",
    "    print(f\"Trigger with most negative sentiment: {min(trigger_avg_sentiment.items(), key=lambda x: x[1])[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
